diff --git a/a.py b/b.py
index e054330..b5939e9 100644
--- a/a.py
+++ b/b.py
@@ -1291,22 +1291,8 @@
     nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)
     nbatches_test = len(test_ld)
 
-    ln_emb = train_data.counts
-    # enforce maximum limit on number of vectors per embedding
-    if args.max_ind_range > 0:
-        ln_emb = np.array(
-            list(
-                map(
-                    lambda x: x if x < args.max_ind_range else args.max_ind_range,
-                    ln_emb,
-                )
-            )
-        )
-    else:
-        ln_emb = np.array(ln_emb)
-    m_den = train_data.m_den
-    ln_bot[0] = m_den
-
+    m_den = ln_bot[0]
+    ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep='-')
     args.ln_emb = ln_emb.tolist()
 
     ### parse command line arguments ###
@@ -1436,7 +1436,7 @@
     global data_buffer
     data_buffer = buffer_num * [None]
     global data_iter
-    data_iter = iter(train_ld)
+    data_iter = iter(test_ld) if args.inference_only else iter(train_ld)
     buffer_num = buffer_num if buffer_num <= nbatches else nbatches
     # data_load_begin = time.time()
     load_data(buffer_num, args.bf16)
#@@ -1504,7 +1504,7 @@
#         if training_record[0] == 0:
#             print("num-batches larger than warm up iters, please increase num-batches or decrease warmup iters")
#             exit()
#-        total_samples = training_record[1] * args.mini_batch_size
#+        total_samples = training_record[1] * (args.mini_batch_size // (ext_dist.my_size if ext_dist.my_size > 1 else 1))
#         throughput = total_samples / training_record[0] * 1000
#         print("Throughput: {:.3f} fps".format(throughput))