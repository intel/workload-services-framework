# bertlarge-pytorch-xeon-public-inference

#
# Apache v2 license
# Copyright (C) 2023 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
#
ARG RELEASE

FROM ai-common-img${RELEASE} as ai_common
FROM bertlarge-pytorch-xeon-public-inference-dataset${RELEASE} as inference_data
FROM bertlarge-pytorch-xeon-public-model${RELEASE} as model
FROM bertlarge-pytorch-xeon-public-benchmark${RELEASE} as benchmark

FROM pytorch-intel-public${RELEASE}

SHELL ["/bin/bash", "-c"]

COPY --from=ai_common /home/ai_common /home/workspace/ai_common
COPY --from=inference_data /home/dataset /home/dataset
COPY --from=model /home/bert_squad_model /home/bert_squad_model
COPY --from=benchmark /home/workspace /home/workspace

WORKDIR /home/workspace

ENV MODEL_DIR="/home/workspace"
ENV FINETUNED_MODEL="/home/bert_squad_model"
ENV EVAL_DATA_FILE="/home/dataset/dev-v1.1.json"
ENV EVAL_SCRIPT="/home/workspace/quickstart/language_modeling/pytorch/bert_large/inference/cpu/transformers/examples/legacy/question-answering/run_squad.py"
# Change to offline mode to prevent from access to huggingface.co
ENV TRANSFORMERS_OFFLINE=1
ENV HF_DATASETS_OFFLINE=1

# enable ipex for squad
RUN source activate base && \
    cd quickstart/language_modeling/pytorch/bert_large/inference/cpu && \
    git clone https://github.com/huggingface/transformers.git && \
    cd transformers && \
    git checkout v4.18.0 && \
    git apply ../enable_ipex_for_squad.diff && \
    pip install -e ./ && \
    pip install tensorboard tensorboardX

# Warmup and prepare data in advance
RUN /root/anaconda3/bin/python -u ${EVAL_SCRIPT} \
                               --per_gpu_eval_batch_size=16 --perf_run_iters=10 --benchmark --model_type=bert --model_name_or_path=/home/bert_squad_model \
                               --tokenizer_name=/home/bert_squad_model --do_eval --do_lower_case --predict_file=/home/dataset/dev-v1.1.json \
                               --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=/tmp/ --perf_begin_iter=1 --use_jit \
                               --int8_config=/home/workspace/quickstart/language_modeling/pytorch/bert_large/inference/cpu/configure.json --int8

COPY run_test.sh .

RUN mkfifo /export-logs

CMD ( ./run_test.sh; \
    echo $? > status) 2>&1 | tee benchmark_${MODE}_${TOPOLOGY}_${PRECISION}_${FUNCTION}_${DATA_TYPE}_$(date +"%m-%d-%y-%H-%M-%S").log && \
    tar cf /export-logs status $(find . -name "*.log") && \
    sleep infinity