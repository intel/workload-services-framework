# bertlarge-pytorch-xeon-public-inference-24.04
ARG RELEASE
#
# Apache v2 license
# Copyright (C) 2023 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
#

FROM ai-common-img${RELEASE} as ai_common
FROM bertlarge-pytorch-xeon-public-model-24.04${RELEASE} as model
FROM bertlarge-pytorch-xeon-public-benchmark-24.04${RELEASE} as benchmark
FROM bertlarge-pytorch-xeon-public-inference-dataset-24.04${RELEASE} as inference_data

FROM pytorch-intel-public-24.04${RELEASE}

SHELL ["/bin/bash", "-c"]

COPY --from=ai_common /home/ai_common /home/workspace/ai_common
COPY --from=inference_data /home/dataset /home/dataset
COPY --from=model /home/bert_squad_model /home/bert_squad_model
COPY --from=benchmark /home/workspace /home/workspace
COPY enable_ipex_for_transformers.diff /home/workspace
WORKDIR /home/workspace

ENV MODEL_DIR="/home/workspace"
ENV FINETUNED_MODEL="/home/bert_squad_model"
ENV EVAL_DATA_FILE="/home/dataset/dev-v1.1.json"
ENV EVAL_SCRIPT="/home/workspace/models_v2/pytorch/bert_large/inference/cpu/transformers/examples/legacy/question-answering/run_squad.py"
# Change to offline mode to prevent from access to huggingface.co
ENV TRANSFORMERS_OFFLINE=1
ENV HF_DATASETS_OFFLINE=1
ARG TRANSFORMERS_VER="v4.36.0"
ARG TRANSFORMERS_REPO="https://github.com/huggingface/transformers.git"

# enable ipex for squad
RUN source activate base && \
    cd models_v2/pytorch/bert_large/inference/cpu && \
    git clone ${TRANSFORMERS_REPO} && \
    cd transformers && \
    git checkout ${TRANSFORMERS_VER} && \
    git apply ../../../../../../enable_ipex_for_transformers.diff && \
    pip install -e ./ && \
    pip install tensorboard tensorboardX

# Warmup and prepare data in advance
RUN /root/anaconda3/bin/python -u ${EVAL_SCRIPT} \
                               --per_gpu_eval_batch_size=16 --perf_run_iters=10 --benchmark --model_type=bert --model_name_or_path=/home/bert_squad_model \
                               --tokenizer_name=/home/bert_squad_model --do_eval --do_lower_case --predict_file=/home/dataset/dev-v1.1.json \
                               --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=/tmp/ --perf_begin_iter=1 --ipex --use_jit \
                               --int8_config=/home/workspace/models_v2/pytorch/bert_large/inference/cpu/configure.json --int8 --int8_bf16

COPY run_test.sh .

RUN mkfifo /export-logs

CMD ( ./run_test.sh; \
    echo $? > status) 2>&1 | tee benchmark_${MODE}_${TOPOLOGY}_${PRECISION}_${FUNCTION}_${DATA_TYPE}_$(date +"%m-%d-%y-%H-%M-%S").log && \
    tar cf /export-logs status $(find . -name "*.log") && \
    sleep infinity
