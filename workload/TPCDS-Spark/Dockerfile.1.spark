# hadoop-with-spark

#
# Apache v2 license
# Copyright (C) 2023 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
#
# os
ARG OS_VER=8.5
ARG OS_IMAGE=rockylinux
FROM ${OS_IMAGE}:${OS_VER}

# install dependency
RUN yum install -y dnf-plugins-core && \
    dnf clean all && \
    rm -r /var/cache/dnf && \
    dnf -y upgrade && \
    dnf config-manager --set-enabled powertools && \
    yum install -y wget gcc make flex bison byacc maven git bc patch zlib telnet wget openssh-clients openssh-server net-tools procps-ng nc

# jdk install
ARG JDK_VER=11.0.15
ARG OPENJDK11_VER=11.0.15_10
ARG OPENJDK11_DIR=jdk-11.0.15+10
ARG OPENJDK11_FILE_NAME=OpenJDK11U-jdk_x64_linux_hotspot_${OPENJDK11_VER}.tar.gz
ARG JDK_PACKAGE=https://github.com/adoptium/temurin11-binaries/releases/download/${OPENJDK11_DIR}/${OPENJDK11_FILE_NAME}
RUN wget ${JDK_PACKAGE} && \
    tar xf ${OPENJDK11_FILE_NAME} && \
    mv /jdk*/ /jdk && \
    rm -rf ${OPENJDK11_FILE_NAME}

ENV JAVA_HOME=/jdk
ENV PATH=$PATH:$JAVA_HOME/bin

# tpcds install
ARG TPCDS_KIT_VER=45ab85a
ARG TPCDS_KIT_REPO=https://github.com/databricks/tpcds-kit.git

RUN cd /tmp && \
    git clone ${TPCDS_KIT_REPO} && \
    cd tpcds-kit && \
    git checkout ${TPCDS_KIT_VER} && \
    cd tools && \
    sed -i 's/^BASE_CFLAGS.*/BASE_CFLAGS = -fcommon -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -DYYDEBUG/g' Makefile.suite && \
    make clean && \
    make OS=LINUX -f Makefile.suite

# spark perf install
ARG SPARK_VER=3.2.2
ARG SPARK_SQL_PERF_VER=28d8819
ARG SPARK_SQL_PERF_REPO=https://github.com/databricks/spark-sql-perf.git

RUN rm -f /etc/yum.repos.d/bintray-rpm.repo || true && \
    curl -L https://www.scala-sbt.org/sbt-rpm.repo > sbt-rpm.repo && \
    mv sbt-rpm.repo /etc/yum.repos.d/ && \
    yum install -y sbt && \
    cd /tmp && \
    git clone ${SPARK_SQL_PERF_REPO} && \
    cd spark-sql-perf && \
    git checkout ${SPARK_SQL_PERF_VER} && \
    sed -i "s/^sparkVersion.*/sparkVersion := \"$SPARK_VER\"/g" build.sbt && \
    cat build.sbt
ENV MAVEN_OPTS="-Dhttp.proxyHost=${HTTP_PROXY_ADDRESS} -Dhttp.proxyPort=${HTTP_PROXY_PORT} \
                -Dhttps.proxyHost=${HTTPS_PROXY_ADDRESS} -Dhttps.proxyPort=${HTTPS_PROXY_PORT}"

RUN cd /tmp/spark-sql-perf && \
    sbt +package && \
    mkdir /tmp/lib && \
    cp /tmp/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar /tmp/lib/spark-sql-perf.jar

COPY package /tmp/package

RUN cd /tmp/package && \
    mkdir lib && \
    cp /tmp/lib/spark-sql-perf.jar lib && \
    sbt +package && \
    cp /tmp/package/target/scala-2.12/tpcds-spark-benchmark_2.12-1.2.jar /tmp/lib/tpcds-spark-benchmark.jar

# hadoop install
ARG HADOOP_VER=3.2.2
ARG HADOOP_PACKAGE=https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VER}/hadoop-${HADOOP_VER}.tar.gz
RUN wget -O hadoop-${HADOOP_VER}.tar.gz ${HADOOP_PACKAGE} && \
    tar -xf /hadoop-${HADOOP_VER}.tar.gz && \
    mv /hadoop-${HADOOP_VER} /usr/local/hadoop && \
    rm /hadoop-${HADOOP_VER}.tar.gz

# spark install
ARG SPARK_VER=3.2.2
ARG SPARK_PACKAGE=https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-without-hadoop.tgz
RUN wget -O spark-${SPARK_VER}-bin-without-hadoop.tgz ${SPARK_PACKAGE} && \
    tar -xf /spark-${SPARK_VER}-bin-without-hadoop.tgz && \
    mv /spark-${SPARK_VER}-bin-without-hadoop /usr/local/spark && \
    rm /spark-${SPARK_VER}-bin-without-hadoop.tgz && \
    cp /tmp/lib/spark-sql-perf.jar /usr/local/spark/jars/spark-sql-perf.jar && \
    cp /tmp/lib/tpcds-spark-benchmark.jar /usr/local/spark/jars/tpcds-spark-benchmark.jar

ENV JAVA_HOME=/jdk
ENV SPARK_HOME=/usr/local/spark
ENV HADOOP_HOME=/usr/local/hadoop
ENV HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$SPARK_HOME/bin
ENV HDFS_NAMENODE_USER=root
ENV HDFS_DATANODE_USER=root
ENV HDFS_SECONDARYNAMENODE_USER=root
ENV YARN_RESOURCEMANAGER_USER=root
ENV YARN_NODEMANAGER_USER=root

RUN mkdir ~/.ssh && \
    ssh-keygen -t rsa -P '' -f /etc/ssh/ssh_host_rsa_key  && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    mkdir -p ~/hdfs/namenode && \
    mkdir -p ~/hdfs/datanode && \
    mkdir $HADOOP_HOME/logs

# config
COPY hadoop_config/core-site.xml $HADOOP_CONF_DIR/core-site.xml
COPY hadoop_config/hdfs-site.xml $HADOOP_CONF_DIR/hdfs-site.xml
COPY hadoop_config/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml
COPY hadoop_config/capacity-scheduler.xml $HADOOP_CONF_DIR/capacity-scheduler.xml
COPY hadoop_config/hadoop-env.sh $HADOOP_CONF_DIR/hadoop-env.sh
COPY hadoop_config/spark-default.conf.template $SPARK_HOME/conf/spark-default.conf.template

# run test
RUN mkfifo /export-logs

COPY run_test.sh /
USER root
CMD (/run_test.sh; echo $? > status) 2>&1 | tee output.logs && \
    tar cf /export-logs status output.logs /tmp/spark-logs && \
    sleep infinity
