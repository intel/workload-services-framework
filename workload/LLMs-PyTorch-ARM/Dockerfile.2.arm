# llms-pytorch-arm

#
# Apache v2 license
# Copyright (C) 2023 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
#
ARG OS_VER=24.04
ARG OS_IMAGE=ubuntu
FROM ${OS_IMAGE}:${OS_VER}

RUN apt update && \
    apt full-upgrade -y && \
    DEBIAN_FRONTEND=noninteractive apt install --no-install-recommends -y \
    sudo \
    ca-certificates \
    git \
    curl \
    wget \
    vim \
    numactl \
    gcc-12 \
    g++-12 \
    make
RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 100 && \
    update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-12 100 && \
    update-alternatives --install /usr/bin/cc cc /usr/bin/gcc 100 && \
    update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++ 100

# Prepare the Conda environment
ARG PYTHON_VER="3.10"
ARG PYTHON_REPO="conda"
ARG MINIFORGE_VER="24.3.0-0"
ARG MINIFORGE_REPO="https://github.com/conda-forge/miniforge/releases/download"

RUN wget --progress=bar:force:noscroll \
https://github.com/conda-forge/miniforge/releases/download/24.3.0-0/Mambaforge-24.3.0-0-Linux-aarch64.sh -O miniforge.sh && \
chmod +x miniforge.sh && \
./miniforge.sh -b -p /root/miniforge && \
rm ./miniforge.sh

ENV CONDA_PREFIX=/root/miniforge
ENV PATH="/root/miniforge/bin:${PATH}"
RUN conda create -n llm python=${PYTHON_VER} -y

RUN mkdir -p /home/workspace
WORKDIR /home/workspace

SHELL ["/bin/bash", "-c"]

ARG IntelAI_MODELS_REPO="https://github.com/intel/ai-reference-models"
ARG IntelAI_MODELS_VER="v3.3"
ARG MODEL_ROOT="/home/workspace/pytorch_model"

# Clone the repository and checkout the specified version
RUN source activate llm && \
    git clone ${IntelAI_MODELS_REPO} ${MODEL_ROOT} && \
    cd ${MODEL_ROOT} && \
    git checkout ${IntelAI_MODELS_VER}

# Download the prompt.json file
RUN source activate llm && \
    wget -P ${MODEL_ROOT}/models_v2/pytorch/gptj/inference/cpu https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/prompt.json


# https://github.com/aws/aws-graviton-getting-started/blob/main/machinelearning/pytorch.md#runtime-configurations-for-optimal-performance
# OMP_NUM_THREADS is set by torch.backends.xeon.run_cpu --ncores_per_instance
ENV OMP_PROC_BIND=false
ENV OMP_PLACES=cores
ENV LRU_CACHE_CAPACITY=1024
ENV THP_MEM_ALLOC_ENABLE=1
ENV TORCHINDUCTOR_FREEZING=1


ARG TRANSFORMERS_SG_VER="0.0.5"
ARG TRANSFORMERS_SG_REPO="pip"
ARG TIKTOKEN_VER="0.6.0"
ARG TIKTOKEN_REPO="pip"
# ARG BITSANDBYTES_VER="0.43.0"
# ARG BITSANDBYTES_REPO="pip"
ARG NUMPY_VER="1.26.1"
ARG NUMPY_REPO="pip"
ARG PEFT_VER="0.11.1"
ARG PEFT_REPO="pip"
ARG DATASET_VER="2.20.0"
ARG DATASET_REPO="pip"
RUN source activate llm && pip install transformers_stream_generator==${TRANSFORMERS_SG_VER} \
    tiktoken==${TIKTOKEN_VER} numpy==${NUMPY_VER} peft==${PEFT_VER} datasets==${DATASET_VER}
    
# download lambada dataset in cache 
RUN apt-get update && apt-get install -y ca-certificates && update-ca-certificates
RUN source activate llm && \
    python -c "from datasets import load_dataset; full_dataset = load_dataset(\"NeelNanda/pile-10k\",trust_remote_code=True)" && \
    python -c "from datasets import load_dataset; full_dataset = load_dataset(\"EleutherAI/lambada_openai\",trust_remote_code=True)"

